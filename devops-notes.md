## Dockerfileüê≥

```dockerfile
#1
FROM golang:1.22.5-alpine

#2
WORKDIR /journey

#3¬∫
COPY go.mod go.sum ./

RUN go mod download && go mod verify

#4¬∫
COPY . .

#5¬∫
#WORKDIR /outrodir/app

#6¬∫
RUN go build -o ./bin/journey .

#7¬∫
EXPOSE 8080

#8¬∫
ENTRYPOINT [ "/journey/bin/journey" ]
```

**1¬∫** - FROM √© qual √© a linguagem que utilizamos no container que vamos criar, "de onde vamos partir", ele busca no dockerhub.
<br/> - Sempre utilizar uma tag de vers√£o espec√≠fica, olhar se tem muitos CVE's na vers√£o escolhida.
<br/> - Priorizar vers√£o alpine, que √© menor/mais enxuta, possui uma superf√≠cie de ataque menor.

**2¬∫** - Diret√≥rio de trabalho, se n√£o definido, √© adotado o diret√≥rio raiz, que n√£o √© uma boa pr√°tica. Ele cria a pasta com o nome definido.

**3¬∫** - Copiando os arquivos do projeto go.mod e go.sum respons√°veis por gerenciar as depend√™ncias do projeto, para dentro do container, jogando na raiz da nossa pasta `journey`.
<br> - Tendo os dois arquivos, vamos conseguir executar o comando que realiza esse processo. Para executar comandos no dockerfile utilizamos o comando **RUN**.

**4¬∫** - Ap√≥s instalarmos todas as depend√™ncias, precisamos buildar nossa aplica√ß√£o, por√©m nossos dados ainda n√£o est√£o dentro do container, para jogar tudo que temos no nosso projeto para o container, utilizamos o comando **COPY**, onde o primeiro `.` √© para pegar tudo que temos na raiz do nosso projeto, e o segundo `.` jogar para dentro da raiz do docker, nesse caso no nosso WORKDIR /journey.

**5¬∫** - √â poss√≠vel mudar de diret√≥rio a qualquer momento.

**6¬∫** - Executando o comando de build, onde o `-o` define o output, por ser um bin√°rio vamos definir que ele vai ser gerado e adicionado na pasta /bin/journey, lembrando que o caminho `./bin/journey` sim precisa do `.`, pois ele faz refer√™ncia ao diret√≥rio local(journey), e o segundo par√¢metro √© o `.` que faz refer√™ncia ao arquivo .go que precisa ser buildado, no caso journey.go.

**7¬∫** - A porta que ser√° exposta pelo container, no caso a 8080 por ser onde nossa aplica√ß√£o √© executada, se n√£o expormos a porta, a aplica√ß√£o vai executar sem ser poss√≠vel acessar.

**8¬∫** - Entrypoint √© o que esse container vai executar, qual o caminho do execut√°vel da aplica√ß√£o que desejamos executar no container.

---

## Buildando dockerfileüèó

Comando docker para criar imagem
```
docker build -t api-journey:v1 .
```

Podemos listar as imagens
```
docker image ls ou docker images
```

Executar a cria√ß√£o do nosso container
```
docker run --name api-journey -d -p 8080:8080 api-journey:v1
```
> Comando `-d` √© para rodar o container detached do terminal.

Verificar os containers rodando
```
docker ps -a
```

Se nosso container n√£o estiver rodando, podemos ver seu log utilizando o container ID
```
docker logs 514aa176536b
```

---

## Multi-Stage build‚ú®

Podemos ver que o tamanho da nossa imagem criada est√° bem alto, muito do que temos em nossa imagem s√£o arquivos do nosso projeto, por√©m, no final do dia, s√≥ precisamos do bin√°rio.

Temos uma imagem de API simples com quase 600MB de espa√ßo.

![tamanho-imagem-v1](https://github.com/user-attachments/assets/10d296fb-f1b7-4ed7-bd09-641d827c926d)

Para resolver isso podemos alterar nosso dockerfile para ter v√°rias etapas, como build e execu√ß√£o.

```dockerfile
#1¬∫
FROM golang:1.22.5-alpine as builder

WORKDIR /app

COPY go.mod go.sum ./

RUN go mod download && go mod verify

COPY . .

RUN go build -o /bin/journey .

#2¬∫
FROM scratch

WORKDIR /app

#3¬∫
COPY --from=builder /bin/journey .

EXPOSE 8080

ENTRYPOINT [ "./journey" ]
```

**1¬∫** - Demos um alias a nosso processo, tudo que tiver de opera√ß√µes at√© o pr√≥ximo `FROM` eu estou chamando de `builder`.

**2¬∫** - Nesse segundo est√°gio, inicio nosso container com base no [scratch](https://hub.docker.com/_/scratch), uma imagem docker que tem apenas o b√°sico para um sistema ser executado.

**3¬∫** - Conseguimos se comunicar entre est√°gios, onde copiamos o que tem dispon√≠vel em /bin/journey do est√°gio `builder` e colamos na raiz do nosso novo est√°gio.

Ap√≥s essa altera√ß√£o, podemos criar nossa imagem novamente e visualizarmos se houve altera√ß√£o no tamanho de nossa imagem.

![tamanho-imagem-v2](https://github.com/user-attachments/assets/d137bb58-96c3-4b16-8950-49d026535bff)

Tivemos uma diminui√ß√£o de 96.95% no tamanho da nossa imagem.

---

## CI com github actions

√â poss√≠vel criar esse arquivo pelo github, mas caso queiramos criar a pasta e o arquivo `.github/workflows/main.yml` tamb√©m √© poss√≠vel.

```yaml
#1¬∫
name: CI

#2¬∫
on:
  push:
    branches:
      - master

#3¬∫
jobs:
  #4¬∫  
  build-and-push:
    name: "Build and Push"
    runs-on:  ubuntu-latest

    #5¬∫
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Build docker image
        run: docker build -t sandrolax/api-journey:latest .

```

**1¬∫** - Nome do nosso workflow

**2¬∫** - Define quando √© trigado nosos workflow, no caso, quando houver um push na branch master.

**3¬∫** - O job √© uma m√°quina em execu√ß√£o e essa m√°quina tem v√°rios steps, podemos definir v√°rios jobs com v√°rios steps(teste/build/etc).

**4¬∫** - Nome do meu job e onde ele vai ser executado.

**5¬∫** - Steps s√£o os passos que desejo realizar quando o job processar, no nosso caso fazemos o `Checkout` que √© um [actions](https://github.com/actions/checkout)(steps pr√©-prontos) que basicamente √© um check out da branch no workspace, ap√≥s isso realizamos o step de buildar a imagem da nossa api.

Como o objetivo √© em breve enviar para o container registry do dockerhub, precisamos colocar o nosso nome de usu√°rio do dockerhub na frente do nome da imagem.

---

### Melhorias na action

**Gerar TAG imagem com base hash do commit**: Para isso criamos um step anterior a cria√ß√£o da imagem e utilizando a v√°riavel $GITHUB_SHA que est√° presente no contexto, tempos acesso ao hash, ap√≥s isso pegamos os 7 primeiros caracteres.

step:
```yaml
- name: Generate SHA
  id: generate_sha
  run: |
    SHA=$(echo $GITHUB_SHA | head -c7)
    echo "sha=$SHA" >> $GITHUB_OUTPUT
```

* Id para identificar os valores criados nesse passo.
* O pipe √© utilizado para definirmos comandos que tenham mais de uma linha.
* SHA recebe os 7 primeiros caracteres do hash do commit
* Criamos uma vari√°vel para adicionar o valor de SHA no output desse step. Todo step tem o output do anterior, uma maneira centralizada de ir passando informa√ß√µes entre os steps.

Ap√≥s criado e adicionado na vari√°vel `GITHUB_OUTPUT`, podemos utilizar para definir a tag da cria√ß√£o da nossa imagem. Abaixo temos um exemplo de como acessar esse valor no step de build.

```yaml
- name: Build docker image
  run: docker build -t sandrolax/api-journey:${{ steps.generate_sha.outputs.sha }} .
```

**Login no container registry**: Para fazer isso, vamos utilizar o action [Docker Login](https://github.com/marketplace/actions/docker-login), nele precisamos passar o usu√°rio e senha(token) do registry que vamos utilizar.

Utilizando o DockerHub passo o nome do meu user e o token √© gerado no dockerhub em Account settings > Security > New Access Token.

Para utilizar essas informa√ß√µes, como uma boa pr√°tica, vamos utilizar os secrects do github para setar os dados, isso est√° dispon√≠vel em Settings(Do reposit√≥rio) > Security > Actions > New Repository secret.

Abaixo o step criado:
```yaml
- name: Login to Docker Hub
  uses: docker/login-action@v3
  with:
    username: ${{ secrets.DOCKERHUB_USERNAME }}
    password: ${{ secrets.DOCKERHUB_TOKEN }}
```

**Enviando imagem para container registry**: Criamos o step e adicionamos o docker push, comando que envia a imagem criada para o dockerhub. Quanto √† tag podemos defini-la utilizando o comando docker tag.

Step criado:
```yaml
- name: Push to registry
  run: | 
    docker push sandrolax/api-journey:${{ steps.generate_sha.outputs.sha }}
    docker tag sandrolax/api-journey:${{ steps.generate_sha.outputs.sha }} sandrolax/api-journey:latest
    docker push sandrolax/api-journey:latest
```
> Boa pr√°tica

**Utilizando action para realizar o build e push**: O que fizemos manualmente at√© o momento funciona, por√©m, n√£o √© a melhor pr√°tica e est√° bem verboso. Vamos utilizar a [action ](https://github.com/marketplace/actions/build-and-push-docker-images) para melhorar essa parte do nosso workflow.

Revisando, ficar√° da seguinte forma:
```yaml
- name: Build and push
  uses: docker/build-push-action@v6
  with:
    context: .
    push: true
    tags: |
    sandrolax/api-journey:${{ steps.generate_sha.outputs.sha }}
    sandrolax/api-journey:latest
```

**Steps adicionais**: Poderiamos adicionar tamb√©m um step para realizar os testes unit√°rios, para isso √© necess√°rio ter o go na m√°quina onde roda o job, ent√£o precisamos instalar o go e ent√£o rodar os testes unit√°rios.

Exemplo abaixo:
```
- name: Setup Go
uses: actions/setup-go@v5
with:
    go-version: "1.22.5"

- name: Run tests
run: go test
```

---

## Kubernetes‚öì

### Arquitetura kube

![Components of kube](https://kubernetes.io/images/docs/components-of-kubernetes.svg)

**Node**: S√£o os componentes de trabalho, os n√≥s se comunicam com o control plane atrav√©s do kubelet e tem uma camada de rede que √© o kube-proxy. Conceitos como deployment, pod, deamonSet, service, ingress est√£o nesse componente. Para mais informa√ß√µes, consulte a [documenta√ß√£o](https://kubernetes.io/docs/concepts/architecture/nodes/).
**Control Plane**: √â o que gerencia globalmente nosso cluster, componentes de rede, scheduler, api, etcd(banco chave valor).  √â o cara que, se cair, temos um baita problema. Para mais informa√ß√µes, consulte a [documenta√ß√£o](https://dockerlabs.collabnix.com/kubernetes/beginners/Kubernetes_Control_Plane.html).
**Scheduler**: √â quem tenta alocar nossa aplica√ß√£o em um determinado n√≥. [Documenta√ß√£o](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)

---

### Namespace

√â uma divis√£o l√≥gica para garantir uma melhor organiza√ß√£o na execu√ß√£o dos nossos pods. [Mais detalhes](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/namespace-v1/).

Criando via comand line:
```
kubectl create namespace journey
```

---

### Secret

S√£o objetos onde podemos adicionar dados sens√≠veis, possuem uma estrutura chave/valor e encoda em base64 nossos segredos. Por n√£o ser uma estrutura que criptografa os dados, n√£o √© recomendada a utiliza√ß√£o em produ√ß√£o. Para mais detalhes, acesse a [documenta√ß√£o](https://kubernetes.io/docs/concepts/configuration/secret/).

Exemplo de secret:
```yaml
apiVersion: v1
kind: Service

metadata:
  name: journey-service
  labels:
    app: journey

spec:
  selector:
    app: journey
  type: ClusterIP
  ports:
    - name: journey-service
      port: 80
      targetPort: 8080
      protocol: TCP
```

Para aplicar o secret podemos utilizar o comando:
```
kubectl apply -f k8s/secret.yaml -n journey
```

> Realizar no diret√≥rio que possui o yaml e definir o namespace

---

### Deployment

√â a forma declarativa de definir o funcionamento de um replicaset e seus respectivos pods. Nele podemos utilizar outros recursos criados em nosso cluster, como, por exemplo, os secrets. Para mais sobre, [documenta√ß√£o](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/).

Exemplo deployment:
```yaml
apiVersion: apps/v1
kind: Deployment

metadata:
  name: journey-deployment
  labels:
    app: journey

spec:
  replicas: 5
  selector:
    matchLabels:
      app: journey
  template:
    metadata:
      labels:
        app: journey
    spec:
      containers:
        - name: api-journey
          image: sandrolax/api-journey:4388865
          env:
            - name: JOURNEY_DATABASE_USER
              valueFrom:
                secretKeyRef:
                  name: db-connection
                  key: db_user
            - name: JOURNEY_DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-connection
                  key: db_password
            - name: JOURNEY_DATABASE_HOST
              valueFrom:
                secretKeyRef:
                  name: db-connection
                  key: db_host
            - name: JOURNEY_DATABASE_PORT
              valueFrom:
                secretKeyRef:
                  name: db-connection
                  key: db_port
            - name: JOURNEY_DATABASE_NAME
              valueFrom:
                secretKeyRef:
                  name: db-connection
                  key: db_name
          ports:
            - containerPort: 8080
          resources: 
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 128Mi
```

Para aplicar √© similar a secret:
```
kubectl apply -f k8s -n journey
```

> Por padr√£o, ele busca o arquivo deployment na pasta e o executa.

---

### Service

√â uma maneira de expor a rede do cluster, para conseguirmos acessar a aplica√ß√£o que est√° nos pods, na confirgura√ß√£o do service precisamos definir como ir√° funcionar a rede interna deles, ap√≥s isso realizar um [port-forward](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/) para externalizar a rede interna para o da m√°quina que executa o k8s. Documenta√ß√£o sobre [service](https://kubernetes.io/docs/concepts/services-networking/service/).

Exemplo de service:
```yaml
apiVersion: v1
kind: Service

metadata:
  name: journey-service
  labels:
    app: journey

spec:
  selector:
    app: journey
  type: ClusterIP
  ports:
    - name: journey-service
      port: 80
      targetPort: 8080
      protocol: TCP
```

Aplicando service:
```
kubectl apply -f k8s/service.yaml -n journey
```

Executando o port-forward:
```
kubectl port-forward svc/journey-service 8080:80 -n journey
```

> Sempre importante lembrar de passar o namespace na execu√ß√£o dos comandos

Para mais informa√ß√µes sobre o kube consultar o reposit√≥rio [study-k8s](https://github.com/Sandrolaxx/study-k8s) e tamb√©m a [documenta√ß√£o oficial](https://kubernetes.io/docs/home).

## Helm

Helm √© um gerenciador de pacotes para Kubernetes que facilita o gerenciamento de aplicativos Kubernetes atrav√©s do uso de "charts". Um chart √© um pacote Helm que cont√©m todos os recursos necess√°rios para rodar um aplicativo, incluindo templates de YAML para configura√ß√£o e defini√ß√µes de Kubernetes.

* Charts: S√£o pacotes de Helm que cont√™m todos os arquivos de configura√ß√£o necess√°rios para executar um aplicativo ou servi√ßo em um cluster Kubernetes. Eles podem ser versionados e armazenados em reposit√≥rios.

* Releases: Cada instala√ß√£o de um chart cria uma "release". Uma release √© uma inst√¢ncia de um chart rodando no Kubernetes. Helm gerencia o ciclo de vida dessas releases, permitindo que sejam atualizadas, revertidas e removidas.

* Templates: Helm utiliza arquivos de template para definir a configura√ß√£o dos recursos do Kubernetes. Esses templates s√£o preenchidos com valores espec√≠ficos durante a instala√ß√£o, permitindo a parametriza√ß√£o dos charts.

* Valores (Values): S√£o vari√°veis que podem ser passadas para templates durante a instala√ß√£o ou atualiza√ß√£o de um chart. Eles permitem a personaliza√ß√£o dos recursos Kubernetes definidos pelos templates.

* Reposit√≥rios de Charts: Helm utiliza reposit√≥rios de charts para armazenar e distribuir charts. Reposit√≥rios podem ser p√∫blicos ou privados, permitindo a f√°cil partilha e distribui√ß√£o de pacotes Helm.

### Instala√ß√£o

Comando para realizar a instala√ß√£o:

```
curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh 
chmod 700 get_helm.sh 
./get_helm.sh
```

Comando para criar as configs do k8s com helm:
```
helm create k8s-helm
```

[Aqui](/devops/k8s-helm/values.yaml) temos um exemplo de um arquivo helm j√° configurado.

---

## Terraform

Terraform √© uma ferramenta de c√≥digo aberto desenvolvida pela HashiCorp que permite definir, provisionar e gerenciar a infraestrutura de TI de maneira eficiente e automatizada. Utilizando uma linguagem de configura√ß√£o declarativa chamada HashiCorp Configuration Language (HCL), o Terraform permite que os usu√°rios descrevam a infraestrutura desejada, conhecida como "infraestrutura como c√≥digo" (IaC). Para mais sobre, acessar [documenta√ß√£o](https://developer.hashicorp.com/terraform?product_intent=terraform).

Utilizadas as configura√ß√µes criadas e disponibilizadas no [reposit√≥rio](https://github.com/rocketseat-education/nlw-journey-devops) na pasta iac-cross.

### Instala√ß√£o

Para instalar √© muito simples e a [documenta√ß√£o](https://developer.hashicorp.com/terraform/install) completa √© muito rica. Caso utilize linux basta realizar o comando abaixo.

```
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform
```

### AWS como provider

Para utilizarmos a AWS nos processos de cria√ß√£o de infra com o terraform, precisamos criar nossa conta da AWS, gerar a key e acess_key do IAM, que √© um processo bem simples. Para gerar essas credenciais, vamos em Minha conta > Credenciais de seguran√ßa > Chaves de acesso, criamos uma nova e adicionamos a var envs.

Export das envs utilizadas pelo tf:
```
export AWS_ACCESS_KEY_ID="CREDENCIAL"
export AWS_SECRET_ACCESS_KEY="CREDENCIAL"
export AWS_REGION="us-east-2"
```

Ou tamb√©m podemos executar e passar essas informa√ß√µes:
```
aws config
```

Claro, tamb√©m precisamos instalar a CLI da AWS. Podemos fazer isso com o comando abaixo:

```
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
```

Com isso j√° temos nosso terraform configurado com nosso AWS.

### Criando infra EKS com tf

Criamos o arquivo `terraform.tfvars` para definir as vari√°veis necess√°rias para a configura√ß√£o do EKS.

terraform.tfvars
```
cluster_name="journey"
desired_size=1
max_size=2
min_size=1
prefix="nlw"
retention_days=1
vpc_cidr_block="10.0.0.0/16"
```

Ap√≥s isso podemos rodar o comando `terraform plan` que vai verificar tudo o que precisa ser criado em nosso provider com base na infra que definimos no arquivo `main.tf`.

Para iniciar a cria√ß√£o da nossa infra executamos o comando `terraform apply -auto-approve`

O processo para criar todo o que foi definido pode demorar alguns minutos, ap√≥s finalizado toda a estrutura estar√° criada no cloud provider.

---

## Deploy EKS

Primeiro precisamos atualizar a configura√ß√£o co cluster com o comando:

```
aws eks update-kubeconfig --region us-east-2 --name nlw-journey
```

Ao realizar esse processo localmente o kubectl vai come√ßar a apontar para o cluster da AWS e n√£o mais o local.

Ap√≥s realizado esse processo executamos o comando do helm novamente, agora ele vai criar a estrutura que criamos localmente no cluster da AWS, uma vez que o kubectl est√° apontando para l√°.

Com isso fizemos o deploy manualmente no EKS, mas a ideia n√£o √© essa, sim automatizar esse fluxo, para fazer isso precisamos entender conceitos de gitops.

---

## GitOps e Argo CD

### GitOps

GitOps √© uma abordagem para automa√ß√£o e gerenciamento de infraestrutura e aplica√ß√µes baseada em princ√≠pios de DevOps e infraestrutura como c√≥digo (IaC). Ela utiliza reposit√≥rios Git como a fonte √∫nica de verdade para definir e gerenciar a infraestrutura e as aplica√ß√µes. [O que √© GitOps?](https://www.redhat.com/pt-br/topics/devops/what-is-gitops).

### Argo CD

Argo CD √© uma ferramenta de entrega cont√≠nua (CD) para Kubernetes que utiliza Git como fonte √∫nica de verdade para o estado desejado das aplica√ß√µes. Ele sincroniza automaticamente o estado do cluster Kubernetes com as defini√ß√µes armazenadas em reposit√≥rios Git, garantindo que a infraestrutura e as aplica√ß√µes estejam sempre em conformidade com as configura√ß√µes especificadas no Git.

Instala√ß√£o no cluster de maneira simples:
```
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

Ap√≥s adicionado ao cluster podemos acessar em services o svc `argocd-server` e fazer um port-forward, ao chegar na tela de login a credencial padr√£o √© username `admin` e o password conseguimos encontrar em secrets o `argocd-initial-admin-secret`.

Para realizar a conex√£o do Argo com o git precisamos de dois arquivos, um para definir o login do argo no repo, que √© o [repository.yaml](/devops/deploy-cross/apps/journey/repository.yaml), ele cria um secret com os dados em um formato que o argo consuma e tente realizar o login.

Segundo arquivo √© o [argo.yaml](/devops/deploy-cross/apps/journey/argo.yaml) que define a cria√ß√£o de uma aplica√ß√£o dentro do Argo CD.

### Aten√ß√£o!!!

O argo s√≥ realiza o processo de sync de altera√ß√µes realizadas no caminho definido no spec.source.path do arquivo argo.yaml, demais altera√ß√µes em outros diret√≥rios n√£o acarretaram sync.